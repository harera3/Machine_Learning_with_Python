{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 15\n",
    "---\n",
    "# K-NEAREST NEIGHBORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- KNN is one of the simplest yet most commonly used classifiers in supervised machine learning\n",
    "- Often considered a lazy learner, it doesn’t technically train a model to make predictions\n",
    "- Instead an observation is predicted to be the class of that of the largest proportion of the k nearest observations\n",
    "- In this chapter we will explore how to use scikit-learn to create and use a KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 Finding an Observation’s Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.03800476, 0.55861082, 1.10378283, 1.18556721],\n",
       "        [0.79566902, 0.32841405, 0.76275827, 1.05393502]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Standardize features\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "# Two nearest neighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n",
    "\n",
    "# Create an observation\n",
    "new_observation = [ 1,  1,  1,  1]\n",
    "\n",
    "# Find distances and indices of the observation's nearest neighbors\n",
    "distances, indices = nearest_neighbors.kneighbors([new_observation])\n",
    "\n",
    "# View the nearest neighbors\n",
    "features_standardized[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our solution we used the dataset of Iris flowers. We created an observation, new_observation, with some values and then found the two observations that are closest to our observation. indices contains the locations of the observations in our\n",
    "dataset that are closest, so X[indices] displays the values of those observations. Intuitively, distance can be thought of as a measure of similarity, so the two closest observations are the two flowers most similar to the flower we created.\n",
    "\n",
    "**Measuring the distance**:\n",
    "Minkowski (default)$$\n",
    "d_{minkowski} = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{\\frac{1}{p}}\n",
    "$$\n",
    "- if $p=1$, we get Manhattan distance\n",
    "- if $p=2$, we get Euclidean distance (sklearn's default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49140089, 0.74294782]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find two nearest neighbors based on euclidean distance\n",
    "nearestneighbors_euclidean = NearestNeighbors(\n",
    "n_neighbors=2, metric='euclidean').fit(features_standardized)\n",
    "\n",
    "# View distances\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use kneighbors_graph to create a matrix indicating each observation’s nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find each observation's three nearest neighbors\n",
    "# based on euclidean distance (including itself)\n",
    "nearestneighbors_euclidean = NearestNeighbors(\n",
    "n_neighbors=3, metric=\"euclidean\").fit(features_standardized)\n",
    "\n",
    "# List of lists indicating each observation's 3 nearest neighbors\n",
    "# (including itself)\n",
    "nearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(\n",
    "    features_standardized).toarray()\n",
    "\n",
    "# Remove 1's marking an observation is a nearest neighbor to itself\n",
    "for i, x in enumerate(nearest_neighbors_with_self):\n",
    "    x[i] = 0\n",
    "\n",
    "# View first observation's two nearest neighbors\n",
    "nearest_neighbors_with_self[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are finding nearest neighbors or using any learning algorithm based on distance, it is important to transform features so that they are on the same scale. The reason is because the distance metrics treat all features as if they were on the same\n",
    "scale, but if one feature is in millions of dollars and a second feature is in percentages, the distance calculated will be biased toward the former. In our solution we addressed this potential issue by standardizing the features using StandardScaler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Creating a K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given an observation of unknown class, you need to predict its class based on the class of its neighbors.\n",
    "- If the dataset is not very large, use KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Standardize features\n",
    "X_std = standardizer.fit_transform(X)\n",
    "\n",
    "# Train a KNN classifier with 5 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1).fit(X_std, y)\n",
    "\n",
    "# Create two observations\n",
    "new_observations = [[ 0.75,  0.75,  0.75,  0.75],\n",
    "                    [ 1,  1,  1,  1]]\n",
    "\n",
    "# Predict the class of two observations\n",
    "knn.predict(new_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "In KNN, given an observation $x_u$, with an unknown target class, the algorithm first identifies the k closest observations (sometimes called $x_u$'s neighborhood) based on some distance metric, then these k observations \"vote\" based on their class and the class that wins the vote is $x_u$'s predicted class. More formally, the probability $x_u$ is some class $j$ is:\n",
    "$$\\frac{1}{k} \\sum_{i \\in v}{I(y_i = j)}$$\n",
    "where \n",
    "- $v$ is the $k$ observation in $x_u$'s neighborhood, \n",
    "- $y_i$ is the class of the ith observation, and \n",
    "- $I$ is an indicator function (i.e., 1 is true, 0 otherwise). \n",
    "\n",
    "In scikit-learn we can see these probabilities using predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.6, 0.4],\n",
       "       [0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View probability each observation is one of three classes\n",
    "knn.predict_proba(new_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(new_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default KNeighborsClassifier works how we described previously, with each observation in the neighborhood getting one vote; however, if we set the weights parameter to distance, the closer observations’ votes are weighted more than observations farther away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Identifying the Best Neighborhood Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You want to select the best value for k in a k-nearest neighbors classifier\n",
    "- Use model selection techniques like GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Standardize features\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "# Create a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"standardizer\", standardizer), (\"knn\", knn)])\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "# Create grid search\n",
    "classifier = GridSearchCV(\n",
    "    pipe, search_space, cv=5, verbose=0).fit(features_standardized, target)\n",
    "\n",
    "# Best neighborhood size (k)\n",
    "classifier.best_estimator_.get_params()[\"knn__n_neighbors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4 Creating a Radius-Based Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given an observation of unknown class, you need to predict its class based on the class of all observations within a certain distance\n",
    "- Use `RadiusNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Standardize features\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "# Train a radius neighbors classifier\n",
    "rnn = RadiusNeighborsClassifier(\n",
    "    radius=.5, n_jobs=-1).fit(features_standardized, target)\n",
    "\n",
    "# Create two observations\n",
    "new_observations = [[ 1,  1,  1,  1]]\n",
    "\n",
    "# Predict the class of two observations\n",
    "rnn.predict(new_observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn RadiusNeighborsClassifier is very similar to KNeighborsClassifier, with the exception of two parameters. \n",
    "- First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. \n",
    "- The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius—which itself can often be a useful tool for identifying outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
