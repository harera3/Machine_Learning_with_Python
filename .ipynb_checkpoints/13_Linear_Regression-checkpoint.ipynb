{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 13\n",
    "---\n",
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Linear regression is one of the simplest supervised learning algorithms in our toolkit\n",
    "- It is so simple that it is sometimes not considered machine learning at all!\n",
    "- The fact is that linear regression—and its extensions—continues to be a common and useful method of making predictions when the target vector is a quantitative value (e.g., home price, age)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 Fitting a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** You want to train a model that represents a linear relationship between the feature and target vector.\n",
    "\n",
    "**Solution:** Use a linear regression (in scikit-learn, ${LinearRegression}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 22.485628113468223\n",
      "Coefficients: [-0.35207832  0.11610909]\n",
      "First Target Value: 24000.0\n",
      "First Observation: 24573.366631705547\n",
      "First Coefficient: -352.07831564026765\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load data with only two features\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "# Create linear regression\n",
    "regression = LinearRegression()\n",
    "\n",
    "# Fit the linear regression\n",
    "model = regression.fit(features, target)\n",
    "\n",
    "# View the intercept\n",
    "print('Intercept:', model.intercept_)\n",
    "\n",
    "# View the feature coefficients\n",
    "print('Coefficients:', model.coef_)\n",
    "\n",
    "# First value in the target vector multiplied by 1000\n",
    "print('First Target Value:', target[0]*1000)\n",
    "\n",
    "# Predict the target value of the first observation, multiplied by 1000\n",
    "print('First Observation:', model.predict(features)[0]*1000)\n",
    "\n",
    "# First coefficient multiplied by 1000\n",
    "print('First Coefficient:', model.coef_[0]*1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "- In our dataset, the target value is the median value of a Boston home (in the 1970s) in thousands of dollars\n",
    "- The major advantage of linear regression is its interpretability, in large part because the coefficients of the model are the effect of a one-unit change on the target vector.\n",
    "- For example, the first feature in our solution is the number of crimes per resident.\n",
    "    - Our model’s coefficient of this feature was ~–0.35, meaning that if we multiply this coefficient by 1,000, we have the change in house price for each additional one crime per capita\n",
    "- This says that every single crime per capita will decrease the price of the house by approximately $350!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Handling Interactive Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** You have a feature whose effect on the target variable depends on another feature.\n",
    "\n",
    "**Solution:** Create an interaction term to capture that dependence using scikit-learn’s $PolynomialFeatures$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Observation Feature Values: [6.32e-03 1.80e+01]\n",
      "First Observation Interaction Term: 0.11376\n",
      "First Observation Values: [6.3200e-03 1.8000e+01 1.1376e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load data with only two features\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "# Create interaction term\n",
    "interaction = PolynomialFeatures(\n",
    "    degree=3, include_bias=False, interaction_only=True)\n",
    "features_interaction = interaction.fit_transform(features)\n",
    "\n",
    "# Create linear regression\n",
    "regression = LinearRegression()\n",
    "\n",
    "# Fit the linear regression\n",
    "model = regression.fit(features_interaction, target)\n",
    "\n",
    "# View the feature values for first observation\n",
    "print('First Observation Feature Values:', features[0])\n",
    "\n",
    "# Import library\n",
    "import numpy as np\n",
    "\n",
    "# For each observation, multiply the values of the first and second feature\n",
    "interaction_term = np.multiply(features[:, 0], features[:, 1])\n",
    "\n",
    "# View interaction term for first observation\n",
    "print('First Observation Interaction Term:', interaction_term[0])\n",
    "\n",
    "# View the values of the first observation\n",
    "print('First Observation Values:', features_interaction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "- Sometimes a feature’s effect on our target variable is at least partially dependent on another feature\n",
    "- We can account for interaction effects by including a new feature comprising the product of corresponding values from the interacting features\n",
    "- In our solution, we used a dataset containing only two features. \n",
    "    - We printed the first observation’s values for each of those features above\n",
    "    - An interaction term was created by simply multiplying those two values together for every observation\n",
    "    - We printed the interaction term for the first observation above\n",
    "- However, while often we will have a substantive reason for believing there is an interaction between two features, sometimes we will not. \n",
    "- In those cases it can be useful to use scikit-learn’s PolynomialFeatures to create interaction terms for all combinations of features. \n",
    "- We can then use model selection strategies to identify the combination of features and interaction terms that produce the best model. \n",
    "- To create interaction terms using PolynomialFeatures, there are three important parameters we must set. \n",
    "    - Most important, $interaction-only=True$ tells Polynomial Features to only return interaction terms (and not polynomial features, which we will discuss in Section 13.3). \n",
    "    - By default, PolynomialFeatures will add a feature containing ones called a bias. We can prevent that with $include-bias=False$. \n",
    "    - Finally, the degree parameter determines the maximum number of features to create interaction terms from (in this case we wanted to create an interaction term that is the combination of three features). \n",
    "- We can see the output of PolynomialFeatures from our solution by checking to see if the first observation’s feature values and interaction term value match our manually calculated version\n",
    "    - Printed above as the 'First Observation Values'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Fitting a Nonlinear Relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** You want to model a nonlinear relationship.\n",
    "\n",
    "**Solution:** Create a polynomial regression by including polynomial features in a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First_Observation: [0.00632]\n",
      "First_Observation^2: [3.99424e-05]\n",
      "First_Observation^3: [2.52435968e-07]\n",
      "First_Observation_Values: [6.32000000e-03 3.99424000e-05 2.52435968e-07]\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Load data with one feature\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:1]\n",
    "target = boston.target\n",
    "\n",
    "# Create polynomial features x^2 and x^3\n",
    "polynomial = PolynomialFeatures(degree=3, include_bias=False)\n",
    "features_polynomial = polynomial.fit_transform(features)\n",
    "\n",
    "# Create linear regression\n",
    "regression = LinearRegression()\n",
    "\n",
    "# Fit the linear regression\n",
    "model = regression.fit(features_polynomial, target)\n",
    "\n",
    "# View first observation\n",
    "print('First_Observation:', features[0])\n",
    "\n",
    "# View first observation raised to the second power, x^2\n",
    "print('First_Observation^2:', features[0]**2)\n",
    "\n",
    "# View first observation raised to the third power, x^3\n",
    "print('First_Observation^3:', features[0]**3)\n",
    "\n",
    "# View the first observation's values for x, x^2, and x^3\n",
    "print('First_Observation_Values:', features_polynomial[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "- Polynomial regression is an extension of linear regression to allow us to model non‐linear relationships by adding add polynomial features to the model\n",
    "- The linear regression does not “know” that the $x^2$ is a quadratic transformation of $x$. It just considers it one more variable.\n",
    "- The more of these new features we add, the more flexible the “line” created by our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Reducing Variance with Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** You want to reduce the variance of your linear regression model.\n",
    "\n",
    "**Solution:** Use a learning algorithm that includes a shrinkage penalty (also called regularization) like ridge regression and lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [-0.91987132  1.06646104  0.11738487  0.68512693 -2.02901013  2.68275376\n",
      "  0.01315848 -3.07733968  2.59153764 -2.0105579  -2.05238455  0.84884839\n",
      " -3.73066646]\n",
      "Alpha: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "boston = load_boston()\n",
    "features = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# Create ridge regression with an alpha value\n",
    "regression = Ridge(alpha=0.5)\n",
    "\n",
    "# Fit the linear regression\n",
    "model = regression.fit(features_standardized, target)\n",
    "\n",
    "# Load library\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Create ridge regression with three alpha values\n",
    "regr_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "\n",
    "# Fit the linear regression\n",
    "model_cv = regr_cv.fit(features_standardized, target)\n",
    "\n",
    "# View coefficients\n",
    "print('Coefficients:','\\n', model_cv.coef_)\n",
    "\n",
    "# View alpha\n",
    "print('Alpha:', model_cv.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
