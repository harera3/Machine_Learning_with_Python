{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 10\n",
    "---\n",
    "# DIMENSIONALITY REDUCTION USING FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In Chapter 9, we discussed how to reduce the dimensionality of our feature matrix by creating new features with (ideally) similar ability to train quality models but with significantly fewer dimensions. This is called `feature extraction`. In this chapter we will cover an alternative approach: selecting high-quality, informative features and dropping less useful features. This is called `feature selection`.\n",
    "\n",
    "There are three types of feature selection methods:\n",
    "- `Filter`: select the best features by examining their statistical properties\n",
    "- `Wrapper`: use trial and error to find the subset of features that produce models with the highest quality predictions \n",
    "- `Embedded`: select the best feature subset as part or as an extension of a learning algorithm’s training process\n",
    "\n",
    "In this chapter we cover only filter and wrapper feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Thresholding Numerical Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a set of numerical features and want to remove those with low variance (i.e., likely containing little information).\n",
    "- Select a subset of features with variances above a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create features and target\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "# Create high variance feature matrix\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "\n",
    "# View high variance feature matrix\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Variance thresholding (VT) is one of the most basic approaches to feature selection. It is motivated by the idea that features with low variance are likely less interesting (and useful) than features with high variance. VT first calculates the variance of each feature, then it drops all features whose variance does not meet that threshold:\n",
    "$$\n",
    "operatornameVar(x) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "where\n",
    "- $x$ is the feature vector, \n",
    "- $x_i$ is an individual feature value, and \n",
    "- $\\mu$ is that feature’s mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View variances\n",
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features have been standardized (to mean zero and unit variance), then for obvious reasons variance thresholding will not work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize feature matrix\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Caculate variance of each feature\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Thresholding Binary Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a set of binary categorical features and want to remove those with low variance (i.e., likely containing little information).\n",
    "- Select a subset of features with a Bernoulli random variable variance above a given threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
