{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 10\n",
    "---\n",
    "# DIMENSIONALITY REDUCTION USING FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In Chapter 9, we discussed how to reduce the dimensionality of our feature matrix by creating new features with (ideally) similar ability to train quality models but with significantly fewer dimensions. This is called `feature extraction`. In this chapter we will cover an alternative approach: selecting high-quality, informative features and dropping less useful features. This is called `feature selection`.\n",
    "\n",
    "There are three types of feature selection methods:\n",
    "- `Filter`: select the best features by examining their statistical properties\n",
    "- `Wrapper`: use trial and error to find the subset of features that produce models with the highest quality predictions \n",
    "- `Embedded`: select the best feature subset as part or as an extension of a learning algorithm’s training process\n",
    "\n",
    "In this chapter we cover only filter and wrapper feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Thresholding Numerical Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a set of numerical features and want to remove those with low variance (i.e., likely containing little information).\n",
    "- Select a subset of features with variances above a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create features and target\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "# Create high variance feature matrix\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "\n",
    "# View high variance feature matrix\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Variance thresholding (VT) is one of the most basic approaches to feature selection. It is motivated by the idea that features with low variance are likely less interesting (and useful) than features with high variance. VT first calculates the variance of each feature, then it drops all features whose variance does not meet that threshold:\n",
    "$$\n",
    "operatornameVar(x) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "where\n",
    "- $x$ is the feature vector, \n",
    "- $x_i$ is an individual feature value, and \n",
    "- $\\mu$ is that feature’s mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View variances\n",
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features have been standardized (to mean zero and unit variance), then for obvious reasons variance thresholding will not work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize feature matrix\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Caculate variance of each feature\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Thresholding Binary Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a set of binary categorical features and want to remove those with low variance (i.e., likely containing little information).\n",
    "- Select a subset of features with a Bernoulli random variable variance above a given threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create feature matrix with:\n",
    "# Feature 0: 80% class 0\n",
    "# Feature 1: 80% class 1\n",
    "# Feature 2: 60% class 0, 40% class 1\n",
    "features = [[0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]]\n",
    "\n",
    "# Run threshold by variance\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Just like with numerical features, one strategy for selecting highly informative catego‐\n",
    "rical features is to examine their variances. In binary features (i.e., Bernoulli random\n",
    "variables), variance is calculated as:\n",
    "$$\n",
    "Var(x) = p(1 − p)\n",
    "$$\n",
    "where $p$ is the proportion of observations of class 1. Therefore, by setting $p$, we can\n",
    "remove features where the vast majority of observations are one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Handling Highly Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a feature matrix and suspect some features are highly correlated\n",
    "- Use a correlation matrix to check for highly correlated features. If highly correlated features exist, consider dropping one of the correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create feature matrix with two highly correlated features\n",
    "features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# Convert feature matrix into DataFrame\n",
    "dataframe = pd.DataFrame(features)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                          k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "One problem we often run into in machine learning is highly correlated features. If two features are highly correlated, then the information they contain is very similar, and it is likely redundant to include both features. The solution to highly correlated features is simple: remove one of them from the feature set.\n",
    "\n",
    "In our solution, first we create a correlation matrix of all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.976103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034503</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.976103  0.000000\n",
       "1  0.976103  1.000000 -0.034503\n",
       "2  0.000000 -0.034503  1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix\n",
    "dataframe.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we look at the upper triangle of the correlation matrix to identify pairs of highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2\n",
       "0 NaN  0.976103  0.000000\n",
       "1 NaN       NaN  0.034503\n",
       "2 NaN       NaN       NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper triangle of correlation matrix\n",
    "upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we remove one feature from each of those pairs from the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Removing Irrelevant Features for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a categorical target vector and want to remove uninformative features.\n",
    "- If the features are categorical, calculate a chi-square ($χ^2$) statistic between each feature and the target vector\n",
    "- If the features are quantitative, compute the ANOVA F-value between each featureand the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Convert to categorical data by converting data to integers\n",
    "features = features.astype(int)\n",
    "\n",
    "# Select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
