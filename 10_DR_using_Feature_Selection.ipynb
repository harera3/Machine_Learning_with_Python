{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 10\n",
    "---\n",
    "# DIMENSIONALITY REDUCTION USING FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In Chapter 9, we discussed how to reduce the dimensionality of our feature matrix by creating new features with (ideally) similar ability to train quality models but with significantly fewer dimensions. This is called `feature extraction`. In this chapter we will cover an alternative approach: selecting high-quality, informative features and dropping less useful features. This is called `feature selection`.\n",
    "\n",
    "There are three types of feature selection methods:\n",
    "- `Filter`: select the best features by examining their statistical properties\n",
    "- `Wrapper`: use trial and error to find the subset of features that produce models with the highest quality predictions \n",
    "- `Embedded`: select the best feature subset as part or as an extension of a learning algorithm’s training process\n",
    "\n",
    "In this chapter we cover only filter and wrapper feature selection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Thresholding Numerical Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a set of numerical features and want to remove those with low variance (i.e., likely containing little information).\n",
    "- Select a subset of features with variances above a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create features and target\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "# Create high variance feature matrix\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "\n",
    "# View high variance feature matrix\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Variance thresholding (VT) is one of the most basic approaches to feature selection. It is motivated by the idea that features with low variance are likely less interesting (and useful) than features with high variance. VT first calculates the variance of each feature, then it drops all features whose variance does not meet that threshold:\n",
    "$$\n",
    "operatornameVar(x) = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\mu)^2\n",
    "$$\n",
    "where\n",
    "- $x$ is the feature vector, \n",
    "- $x_i$ is an individual feature value, and \n",
    "- $\\mu$ is that feature’s mean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View variances\n",
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features have been standardized (to mean zero and unit variance), then for obvious reasons variance thresholding will not work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize feature matrix\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "# Caculate variance of each feature\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Thresholding Binary Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a set of binary categorical features and want to remove those with low variance (i.e., likely containing little information).\n",
    "- Select a subset of features with a Bernoulli random variable variance above a given threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create feature matrix with:\n",
    "# Feature 0: 80% class 0\n",
    "# Feature 1: 80% class 1\n",
    "# Feature 2: 60% class 0, 40% class 1\n",
    "features = [[0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [0, 1, 0],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 0]]\n",
    "\n",
    "# Run threshold by variance\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Just like with numerical features, one strategy for selecting highly informative catego‐\n",
    "rical features is to examine their variances. In binary features (i.e., Bernoulli random\n",
    "variables), variance is calculated as:\n",
    "$$\n",
    "Var(x) = p(1 − p)\n",
    "$$\n",
    "where $p$ is the proportion of observations of class 1. Therefore, by setting $p$, we can\n",
    "remove features where the vast majority of observations are one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Handling Highly Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a feature matrix and suspect some features are highly correlated\n",
    "- Use a correlation matrix to check for highly correlated features. If highly correlated features exist, consider dropping one of the correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create feature matrix with two highly correlated features\n",
    "features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# Convert feature matrix into DataFrame\n",
    "dataframe = pd.DataFrame(features)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                          k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "One problem we often run into in machine learning is highly correlated features. If two features are highly correlated, then the information they contain is very similar, and it is likely redundant to include both features. The solution to highly correlated features is simple: remove one of them from the feature set.\n",
    "\n",
    "In our solution, first we create a correlation matrix of all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.976103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034503</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.976103  0.000000\n",
       "1  0.976103  1.000000 -0.034503\n",
       "2  0.000000 -0.034503  1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation matrix\n",
    "dataframe.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we look at the upper triangle of the correlation matrix to identify pairs of highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2\n",
       "0 NaN  0.976103  0.000000\n",
       "1 NaN       NaN  0.034503\n",
       "2 NaN       NaN       NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper triangle of correlation matrix\n",
    "upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we remove one feature from each of those pairs from the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Removing Irrelevant Features for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have a categorical target vector and want to remove uninformative features.\n",
    "- If the features are categorical, calculate a chi-square ($χ^2$) statistic between each feature and the target vector\n",
    "- If the features are quantitative, compute the ANOVA F-value between each feature and the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Convert to categorical data by converting data to integers\n",
    "features = features.astype(int)\n",
    "\n",
    "# Select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# Select two features with highest F-values\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of selecting a specific number of features, we can also use SelectPercentile\n",
    "to select the top n percent of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 3\n"
     ]
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# Select top 75% of features with highest F-values\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Chi-square statistics examines the independence of two categorical vectors. That is,\n",
    "the statistic is the difference between the observed number of observations in each\n",
    "class of a categorical feature and what we would expect if that feature was independent (i.e., no relationship) with the target vector:\n",
    "$$\n",
    "χ^2 = \\sum_{i=1}^{n}\\frac{(O_i-E_i)^2}{E_i}\n",
    "$$\n",
    "where Oi is the number of observations in class i and Ei is the number of observations\n",
    "in class i we would expect if there is no relationship between the feature and target\n",
    "vector.\n",
    "\n",
    "By calculating the chi-squared statistic between a feature and the target vector, we obtain a measurement of the independence between the two. \n",
    "- If the target is independent of the feature variable, then it is irrelevant for our purposes because it contains no information we can use for classification. \n",
    "- On the other hand, if the two features are highly dependent, they likely are very informative for training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Recursively Eliminating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You want to automatically select the best features to keep.\n",
    "- Use scikit-learn’s RFECV to conduct recursive feature elimination (RFE) using cross-validation (CV). That is, repeatedly train a model, each time removing a feature until model performance (e.g., accuracy) becomes worse. The remaining features are the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  0.7031277 , -1.16432076,  1.15422032],\n",
       "       [-1.07500204,  2.56148527, -1.3936433 ,  1.44423524],\n",
       "       [ 1.37940721, -1.77039484,  1.02058188,  2.17806632],\n",
       "       ...,\n",
       "       [-0.80331656, -1.60648007, -0.64067478,  0.06790262],\n",
       "       [ 0.39508844, -1.34564911, -1.91468325, -0.05938279],\n",
       "       [-0.55383035,  0.82880112,  0.27936745, -0.75794736]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "                                   n_features = 100,\n",
    "                                   n_informative = 2,\n",
    "                                   random_state = 1)\n",
    "\n",
    "# Create a linear regression\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "# Recursively eliminate features\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have conducted RFE:\n",
    "- we can see the number of features we should keep\n",
    "- we can also see which of those features we should keep\n",
    "- we can even view the rankings of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
