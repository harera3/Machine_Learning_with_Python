{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 11\n",
    "---\n",
    "# MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- In this chapter we will examine strategies for evaluating the quality of models created through our learning algorithms. \n",
    "- It might appear strange to discuss model evaluation before discussing how to create them, but there is a method to our madness. \n",
    "- Models are only as useful as the quality of their predictions, and thus fundamentally our goal is not to create models (which is easy) but to create high-quality models (which is hard). \n",
    "- Therefore, before we explore the myriad learning algorithms, we first set up how we can evaluate the models they produce.\n",
    "\n",
    "## 11.1 Cross-Validating Models\n",
    "**Problem:** we want to evaluate how well our model will work in the real world\n",
    "\n",
    "**Solution:** we will create a pipeline that\n",
    "- preprocesses the data, \n",
    "- trains the model, and then \n",
    "- evaluates it using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693916821849783"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# create features matrix\n",
    "features = digits.data\n",
    "\n",
    "# create target vector\n",
    "target = digits.target\n",
    "\n",
    "# create standardizer\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# create logitic regression object\n",
    "logit = LogisticRegression()\n",
    "\n",
    "# create a pipeline that standardizes, then runs logistic regression\n",
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n",
    "# create k-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# conduct k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             features, # feature matrix\n",
    "                             target, # target vector\n",
    "                             cv=kf, # cross-validation technique,\n",
    "                             scoring=\"accuracy\", # loss function\n",
    "                             n_jobs=-1) # use all CPU cores\n",
    "\n",
    "# calculate mean\n",
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "- Our goal is to evaluate how well our model does on data it has never seen before (e.g., a new customer, a new crime, a new image). \n",
    "- **The validation approach:**\n",
    "    - split data into training set and test set\n",
    "    - set the test set aside and pretend it's never been seen before\n",
    "    - train the model on the training set and teach it how to make the best predictions\n",
    "    - evaluate the model on the testing set and see how it does\n",
    "- The two major weaknesses of the validation approach:\n",
    "    - the performance of the model can be highly dependent on which few observations were selected for the test set. \n",
    "    - Second, the model is not being trained using all the available data, and not being evaluated on all the available data.\n",
    "- **The k-fold cross-validation (KFCV) strategy:**\n",
    "    - data is split into k parts, called \"*folds*\"\n",
    "    - the model is trained using k-1 folds, combined as a training set\n",
    "    - the last fold is used as a test set\n",
    "    - this is repeated k times each time using a different fold as the test set. \n",
    "    - The performance on the model for each of the k iterations is then averaged to produce an overall measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.98888889, 0.96111111, 0.94444444, 0.97777778,\n",
       "       0.98333333, 0.95555556, 0.98882682, 0.97765363, 0.93854749])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KFCV assumes that each observation was created independent from the other, if so it is a good idea to shuffle observations which can be done in scikit-learn by setting shuffle=True.\n",
    "- When using KFCV to evaluate a classifier, it is often beneficial to perform *stratified k-fold* by replacing KFold class with StratifiedKFold. \n",
    "- When using validation sets or cross-validation, it is important to preprocess data based on the training set and then apply those transformations to both the training and test set. The reason is\n",
    "    - we are pretending that the test set is unknown data\n",
    "    - it prevents the leaking of information from test set into the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97777778, 0.98888889, 0.96111111, 0.94444444, 0.97777778,\n",
       "       0.98333333, 0.95555556, 0.98882682, 0.97765363, 0.93854749])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and test sets\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.1, random_state=1)\n",
    "\n",
    "# Fit standardizer to training set\n",
    "standardizer.fit(features_train)\n",
    "\n",
    "# Apply to both training and test sets\n",
    "features_train_std = standardizer.transform(features_train)\n",
    "features_test_std = standardizer.transform(features_test)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = make_pipeline(standardizer, logit)\n",
    "\n",
    "# Do k-fold cross-validation\n",
    "cv_results = cross_val_score(pipeline, # Pipeline\n",
    "                             features, # Feature matrix\n",
    "                             target, # Target vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring=\"accuracy\", # Loss function\n",
    "                             n_jobs=-1) # Use all CPU scores\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross_val_score parameters:\n",
    "- cv determines our cross-validation technique. K-fold is the most common by far\n",
    "- the scoring parameter defines our metric for success\n",
    "- n_jobs=-1 tells scikit-learn to use every core of the computer available to speed up the operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Creating a Baseline Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** we want a simple baseline regression model to compare against our model.\n",
    "\n",
    "**Solution:** use scikit-learn’s DummyRegressor to create a simple model to use as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.001119359203955339"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "boston = load_boston()\n",
    "\n",
    "# Create features\n",
    "features, target = boston.data, boston.target\n",
    "\n",
    "# Make test and training split\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, random_state=0)\n",
    "\n",
    "# Create a dummy regressor\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "\n",
    "# \"Train\" dummy regressor\n",
    "dummy.fit(features_train, target_train)\n",
    "\n",
    "# Get R-squared score\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare, we train our model and evaluate the performance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6354638433202129"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train simple linear regression model\n",
    "ols = LinearRegression()\n",
    "ols.fit(features_train, target_train)\n",
    "\n",
    "# Get R-squared score\n",
    "ols.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Creating a Baseline Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** You want a simple baseline classifier to compare against your model.\n",
    "\n",
    "**Solution:** Use scikit-learn’s DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42105263157894735"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create target vector and feature matrix\n",
    "features, target = iris.data, iris.target\n",
    "\n",
    "# Split into training and test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "features, target, random_state=0)\n",
    "\n",
    "# Create dummy classifier\n",
    "dummy = DummyClassifier(strategy='uniform', random_state=1)\n",
    "\n",
    "# \"Train\" model\n",
    "dummy.fit(features_train, target_train)\n",
    "\n",
    "# Get accuracy score\n",
    "dummy.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the baseline classifier to our trained classifier, we can see the improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Train model\n",
    "classifier.fit(features_train, target_train)\n",
    "\n",
    "# Get accuracy score\n",
    "classifier.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common measure of a classifier’s performance is how much better it is than random guessing. scikit-learn’s DummyClassifier makes this comparison easy. \n",
    "- The strategy parameter gives us a number of options for generating values. There are two strategies. \n",
    "    - *stratified* makes predictions that are proportional to the training set’s target vector’s class proportions \n",
    "    - *uniform* will generate predictions uniformly at random between the different classes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
