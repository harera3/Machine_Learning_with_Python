{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 18\n",
    "---\n",
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bayes' theorem is the premier method for understanding the probability of some event, $P(A|B)$, given some new information, $P(B|A)$, and a prior belief in the probability of the event, P(A):$$\n",
    "P(A | B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "The Bayesian method's popularity has skyrocked in the last decade, more and more rivaling the traditional frequentist applications in academia, government, and business. In machine learning, one applicaiton of Bayes' theorem to classifican comes in the form of the naive Bayes classifier. Naive Bayes classifiers combine a number of desirable qualities in practical machine learning into a single classifier:\n",
    "\n",
    "1. An intuitive approach\n",
    "2. The ability to work with small data\n",
    "3. Low computation costs for training and prediction\n",
    "4. Often solid results in a variety of settigns.\n",
    "\n",
    "Specifically, a naive Bayes classifier is based on:$$\n",
    "P(y | x_1, ..., x_j) = \\frac{P(x_1, ..., x_j | y)P(y)}{P(x_1,...,x_j)}\n",
    "$$\n",
    "where,\n",
    "\n",
    "- $P(y | x_1, ..., x_j)$ is called the *posterior* and is the probability that an observation is class y given observation's values for the j features, $x_1, ..., x_j$\n",
    "- $P(x_1, ..., x_j)$ is called likelihood and is the *likelihood* of an observation's values for features, $x_1, ..., x_j$, given their class y.\n",
    "- $P(y)$ is called the prior and is our belief for the probability of class y before looking at the data\n",
    "- $P(x_1, ..., x_j$) is called the *marginal probability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 Training a Classifier for Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have only continuous features and you want to train a naive Bayes classifier.\n",
    "- Use a Gaussian naive Bayes classifier in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create Gaussian Naive Bayes object\n",
    "classifer = GaussianNB()\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)\n",
    "\n",
    "# Create new observation\n",
    "new_observation = [[ 4,  4,  4,  0.4]]\n",
    "\n",
    "# Predict class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "The most common type of naive Bayes classifier is the Gaussian naive Bayes, in which we assume that the likelihood of the feature values, x, given an observation is of class y, follows a normal distribution:\n",
    "$$\n",
    "p(x_j | y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^2}} e^{-\\frac{(x_j - \\mu_y)^2}{2\\sigma_y^2}}\n",
    "$$\n",
    "where $\\sigma_y^2$ and $\\mu_y$ are the variance and mean values of feature x_j for class y. Because of the assumption of the normal distribution, Gaussian naive Bayes is *best used in cases when all our features are continuous*.\n",
    "\n",
    "One of the interesting aspects of naive Bayes classifiers is that they allow us to assign a prior belief over the respect target classes. We can do this using GaussianNB's priors parameter, which takes in a list of the probabilities assigned to each class of the target vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Naive Bayes object with prior probabilities of each class\n",
    "clf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 Training a Classifier for Discrete and Count Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given discrete or count data, you need to train a naive Bayes classifier.\n",
    "- Use a multinomial naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Brazil is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# Create bag of words\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Create feature matrix\n",
    "features = bag_of_words.toarray()\n",
    "\n",
    "# Create target vector\n",
    "target = np.array([0,0,1])\n",
    "\n",
    "# Create multinomial naive Bayes object with prior probabilities of each class\n",
    "classifer = MultinomialNB(class_prior=[0.25, 0.5])\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)\n",
    "\n",
    "# Create new observation\n",
    "new_observation = [[0, 0, 0, 1, 0, 1, 0]]\n",
    "\n",
    "# Predict new observation's class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
