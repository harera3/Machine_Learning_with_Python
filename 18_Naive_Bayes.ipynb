{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 18\n",
    "---\n",
    "# NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bayes' theorem is the premier method for understanding the probability of some event, $P(A|B)$, given some new information, $P(B|A)$, and a prior belief in the probability of the event, P(A):$$\n",
    "P(A | B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "The Bayesian method's popularity has skyrocked in the last decade, more and more rivaling the traditional frequentist applications in academia, government, and business. In machine learning, one applicaiton of Bayes' theorem to classifican comes in the form of the naive Bayes classifier. Naive Bayes classifiers combine a number of desirable qualities in practical machine learning into a single classifier:\n",
    "\n",
    "1. An intuitive approach\n",
    "2. The ability to work with small data\n",
    "3. Low computation costs for training and prediction\n",
    "4. Often solid results in a variety of settigns.\n",
    "\n",
    "Specifically, a naive Bayes classifier is based on:$$\n",
    "P(y | x_1, ..., x_j) = \\frac{P(x_1, ..., x_j | y)P(y)}{P(x_1,...,x_j)}\n",
    "$$\n",
    "where,\n",
    "\n",
    "- $P(y | x_1, ..., x_j)$ is called the *posterior* and is the probability that an observation is class y given observation's values for the j features, $x_1, ..., x_j$\n",
    "- $P(x_1, ..., x_j)$ is called likelihood and is the *likelihood* of an observation's values for features, $x_1, ..., x_j$, given their class y.\n",
    "- $P(y)$ is called the prior and is our belief for the probability of class y before looking at the data\n",
    "- $P(x_1, ..., x_j$) is called the *marginal probability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 Training a Classifier for Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have only continuous features and you want to train a naive Bayes classifier.\n",
    "- Use a **Gaussian Naive Bayes classifier** in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create Gaussian Naive Bayes object\n",
    "classifer = GaussianNB()\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)\n",
    "\n",
    "# Create new observation\n",
    "new_observation = [[ 4,  4,  4,  0.4]]\n",
    "\n",
    "# Predict class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "The most common type of naive Bayes classifier is the Gaussian naive Bayes, in which we assume that the likelihood of the feature values, x, given an observation is of class y, follows a normal distribution:\n",
    "$$\n",
    "p(x_j | y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^2}} e^{-\\frac{(x_j - \\mu_y)^2}{2\\sigma_y^2}}\n",
    "$$\n",
    "where $\\sigma_y^2$ and $\\mu_y$ are the variance and mean values of feature x_j for class y. Because of the assumption of the normal distribution, Gaussian naive Bayes is *best used in cases when all our features are continuous*.\n",
    "\n",
    "One of the interesting aspects of naive Bayes classifiers is that they allow us to assign a prior belief over the respect target classes. We can do this using GaussianNB's priors parameter, which takes in a list of the probabilities assigned to each class of the target vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Naive Bayes object with prior probabilities of each class\n",
    "clf = GaussianNB(priors=[0.25, 0.25, 0.5])\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 Training a Classifier for Discrete and Count Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given discrete or count data, you need to train a naive Bayes classifier.\n",
    "- Use a **Multinomial Naive Bayes classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                      'Brazil is best',\n",
    "                      'Germany beats both'])\n",
    "\n",
    "# Create bag of words\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "# Create feature matrix\n",
    "features = bag_of_words.toarray()\n",
    "\n",
    "# Create target vector\n",
    "target = np.array([0,0,1])\n",
    "\n",
    "# Create multinomial naive Bayes object with prior probabilities of each class\n",
    "classifer = MultinomialNB(class_prior=[0.25, 0.5])\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)\n",
    "\n",
    "# Create new observation\n",
    "new_observation = [[0, 0, 0, 1, 0, 1, 0]]\n",
    "\n",
    "# Predict new observation's class\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Multinomial naive Bayes works similarly to Gaussian naive Bayes, but the features are assumed to be multinomial distributed. In practice this means that this classifier is *commonly used when we have discrete data*. One of the most common uses is text classification using bag of words or tf-idf approaches.\n",
    "\n",
    "MultinomialNB contains an additive smoothing hyperparameter, alpha, that should be tuned. The default value is 1.0, with 0.0 meaning no smoothing takes place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 Training a Naive Bayes Classifier for Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You have binary feature data and need to train a naive Bayes classifier.\n",
    "- Use a **Bernoulli Naive Bayes classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Create three binary features\n",
    "features = np.random.randint(2, size=(100, 3))\n",
    "\n",
    "# Create a binary target vector\n",
    "target = np.random.randint(2, size=(100, 1)).ravel()\n",
    "\n",
    "# Create Bernoulli Naive Bayes object with prior probabilities of each class\n",
    "classifer = BernoulliNB(class_prior=[0.25, 0.5])\n",
    "\n",
    "# Train model\n",
    "model = classifer.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded). Like its multinomial cousin, Bernoulli naive Bayes is *often used in text classification*, when our feature matrix is simply the presence or absence of a word in a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 Calibrating Predicted Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You want to calibrate the predicted probabilities from naive Bayes classifiers so they are interpretable.\n",
    "- Use **CalibratedClassifierCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create Gaussian Naive Bayes object\n",
    "classifer = GaussianNB()\n",
    "\n",
    "# Create calibrated cross-validation with sigmoid calibration\n",
    "classifer_sigmoid = CalibratedClassifierCV(classifer, cv=2, method='sigmoid')\n",
    "\n",
    "# Calibrate probabilities\n",
    "classifer_sigmoid.fit(features, target)\n",
    "\n",
    "# Create new observation\n",
    "new_observation = [[ 2.6,  2.6,  2.6,  0.4]]\n",
    "\n",
    "# View calibrated probabilities\n",
    "classifer_sigmoid.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "- In Naive Bayes, while the ranking of predicted probabilities for the different target classes is valid, the raw predicted probabilities tend to take on extreme values close to 0 and 1.\n",
    "- To obtain meaningful predicted probabilities we need conduct calibration using the CalibratedClassifierCV class, which creates well-calibrated predicted probabilities using k-fold cross-validation. \n",
    "- In CalibratedClassifierCV the training sets are used to train the model and the test set is used to calibrate the predicted probabilities. The returned predicted probabilities are the average of the k-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.31548432e-04, 9.99768128e-01, 3.23532277e-07]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extreme probabilities from a Gaussian naive Bayes \n",
    "classifer.fit(features, target).predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calibrated probabilities from sigmoid calibration\n",
    "classifer_sigmoid.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
