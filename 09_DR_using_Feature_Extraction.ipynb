{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 9\n",
    "---\n",
    "# DIMENSIONALITY REDUCTION USING FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Not all features are created equal and the goal of feature extraction for dimensionality reduction is to reduce the number of features with only a small loss in our data’s ability to generate high-quality predictions.\n",
    "\n",
    "One downside of the feature extraction techniques we discuss is that the new features we generate will not be interpretable by humans. If we wanted to maintain our ability to interpret our models, dimensionality reduction through feature selection is a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Reducing Features Using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a set of features, you want to reduce the number of features while retaining the variance in the data\n",
    "- Use **`principal component analysis`** with scikit’s `PCA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 54\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "\n",
    "# Create a PCA that will retain 99% of variance\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "\n",
    "# Conduct PCA\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Principal component analysis (PCA) is a popular linear dimensionality reduction technique. PCA projects observations onto the (hopefully fewer) principal components of the feature matrix that retain the most variance. PCA is an unsupervised technique, meaning that it does not use the information from the target vector and instead only considers the feature matrix.\n",
    "\n",
    "PCA is implemented in scikit-learn using the pca method:\n",
    "- if the argument to `n_components` is between 0 and 1, pca returns the minimum amount of features that retain that much variance. It is common to use values of 0.95 and 0.99, meaning 95% and 99% of the variance of the original features has been retained, respectively.\n",
    "- `whiten=True` transforms the values of each principal component so that they have zero mean and unit variance.\n",
    "- `svd_solver=\"randomized\"`, which implements a stochastic algorithm to find the first principal components in often significantly less time.\n",
    "\n",
    "The output of our solution shows that PCA let us reduce our dimensionality by 10 features while still retaining 99% of the information (variance) in the feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Reducing Features When Data Is Linearly Inseparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You suspect you have linearly inseparable data and want to reduce the dimensions.\n",
    "- Use an extension of principal component analysis that uses kernels (**`KernelPCA`**) to allow for non-linear dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 2\n",
      "Reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Create linearly inseparable data\n",
    "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
    "\n",
    "# Apply kernal PCA with radius basis function (RBF) kernel\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)\n",
    "\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
