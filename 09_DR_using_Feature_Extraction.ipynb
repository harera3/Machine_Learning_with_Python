{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 9\n",
    "---\n",
    "# DIMENSIONALITY REDUCTION USING FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Not all features are created equal and the goal of feature extraction for dimensionality reduction is to reduce the number of features with only a small loss in our data’s ability to generate high-quality predictions.\n",
    "\n",
    "One downside of the feature extraction techniques we discuss is that the new features we generate will not be interpretable by humans. If we wanted to maintain our ability to interpret our models, dimensionality reduction through feature selection is a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Reducing Features Using Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given a set of features, you want to reduce the number of features while retaining the variance in the data\n",
    "- Use **`principal component analysis`** with scikit’s `PCA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 64\n",
      "Reduced number of features: 54\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Standardize the feature matrix\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "\n",
    "# Create a PCA that will retain 99% of variance\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "\n",
    "# Conduct PCA\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Principal component analysis (PCA) is a popular linear dimensionality reduction technique. PCA projects observations onto the (hopefully fewer) principal components of the feature matrix that retain the most variance. PCA is an unsupervised technique, meaning that it does not use the information from the target vector and instead only considers the feature matrix.\n",
    "\n",
    "PCA is implemented in scikit-learn using the pca method:\n",
    "- if the argument to `n_components` is between 0 and 1, pca returns the minimum amount of features that retain that much variance. It is common to use values of 0.95 and 0.99, meaning 95% and 99% of the variance of the original features has been retained, respectively.\n",
    "- `whiten=True` transforms the values of each principal component so that they have zero mean and unit variance.\n",
    "- `svd_solver=\"randomized\"`, which implements a stochastic algorithm to find the first principal components in often significantly less time.\n",
    "\n",
    "The output of our solution shows that PCA let us reduce our dimensionality by 10 features while still retaining 99% of the information (variance) in the feature matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Reducing Features When Data Is Linearly Inseparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You suspect you have linearly inseparable data and want to reduce the dimensions.\n",
    "- Use an extension of principal component analysis that uses kernels (**`KernelPCA`**) to allow for non-linear dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 2\n",
      "Reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Create linearly inseparable data\n",
    "features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n",
    "\n",
    "# Apply kernal PCA with radius basis function (RBF) kernel\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)\n",
    "\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion:\n",
    "Standard PCA uses linear projection to reduce the features. If the data is linearly separable (i.e., you can draw a straight line or hyperplane between different classes) then PCA works well. However, if your data is not linearly separable (e.g., you\n",
    "can only separate classes using a curved decision boundary), the linear transformation will not work as well.\n",
    "\n",
    "Kernels allow us to project the linearly inseparable data into a higher dimension where it is linearly separable; this is called the kernel trick. A common kernel to use is the Gaussian `radial basis function kernel` rbf, but other options are the `polynomial kernel` (poly) and `sigmoid kernel` (sigmoid). We can even specify a linear projection (linear), which will produce the same results as standard PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Reducing Features by Maximizing Class Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You want to reduce the features to be used by a classifier.\n",
    "- Try **`linear discriminant analysis`** (LDA) to project the features onto component axes that maximize the separation of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 1\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Load Iris flower dataset:\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create and run an LDA, then use it to transform the features\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda = lda.fit(features, target).transform(features)\n",
    "\n",
    "# Print the number of features\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_lda.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use explained_variance_ratio_ to view the amount of variance explained by each component. In our solution the single component explained over 99% of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9912126])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
